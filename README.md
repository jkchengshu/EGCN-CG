# EGCN-CG
EGCN+CG
# EGCN + Softmax æ¶æ„è¯¦è§£


å®Œæ•´çš„æ¨¡å‹æµç¨‹
```
è¾“å…¥åˆ—ç‰¹å¾ â†’ EGCNç¼–ç å™¨ â†’ èŠ‚ç‚¹åµŒå…¥ â†’ Softmaxåˆ†ç±»å™¨ â†’ å†—ä½™æ€§æ¦‚ç‡
    â†“           â†“           â†“           â†“             â†“
 [features]  [node_emb]  [embeddings] [logits]   [probabilities]
 (N, F)      GCN+GRU     (N, H)      (N, 2)     (N, 2)
```

ğŸ”§ å½“å‰å®ç°çš„æ¶æ„

1. EGCNç¼–ç å™¨éƒ¨åˆ†
```python
# åœ¨ egcn_trainer.py ä¸­
self.model = EGCN(args, activation=F.relu, device=self.device)
```

EGCNçš„åŠŸèƒ½ï¼š
- âœ… å¤šå…³ç³»å›¾å·ç§¯ï¼ˆç…¤ç‚­ã€æ—¶é—´ã€èˆ¹åªå…³ç³»ï¼‰
- âœ… GRUåŠ¨æ€æƒé‡æ›´æ–°
- âœ… ç”ŸæˆèŠ‚ç‚¹åµŒå…¥è¡¨ç¤º

2. Softmaxåˆ†ç±»å™¨éƒ¨åˆ†
```python
# åœ¨ egcn_trainer.py ç¬¬128-133è¡Œ
self.classifier = nn.Sequential(
    nn.Linear(args.layer_2_feats, 16),  # éšè—å±‚
    nn.ReLU(),                          # æ¿€æ´»å‡½æ•°
    nn.Dropout(0.2),                    # é˜²è¿‡æ‹Ÿåˆ
    nn.Linear(16, 2)                    # äºŒåˆ†ç±»è¾“å‡º
).to(self.device)
```

Softmaxåˆ†ç±»å™¨çš„åŠŸèƒ½ï¼š
- âœ… å°†EGCNçš„èŠ‚ç‚¹åµŒå…¥æ˜ å°„åˆ°2ç»´logits
- âœ… é€šè¿‡Softmaxå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
- âœ… è¾“å‡ºå†—ä½™æ€§é¢„æµ‹

3. å‰å‘ä¼ æ’­æµç¨‹
```python
# åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„å®Œæ•´æµç¨‹
def forward_pass(self, features, adj_matrices):
    # 1. EGCNç¼–ç 
    node_embeddings, _ = self.model(adj_matrices, features)
    
    # 2. Softmaxåˆ†ç±»
    logits = self.classifier(node_embeddings)
    probabilities = F.softmax(logits, dim=1)
    predictions = torch.argmax(probabilities, dim=1)
    
    return predictions, probabilities
```

 ğŸ“Š è¾“å‡ºè§£é‡Š
Softmaxè¾“å‡ºæ ¼å¼
```python
# å¯¹äºæ¯ä¸ªåˆ—ï¼ŒSoftmaxè¾“å‡º2ä¸ªæ¦‚ç‡ï¼š
probabilities = [
    [P(å†—ä½™), P(éå†—ä½™)],     # åˆ—1
    [P(å†—ä½™), P(éå†—ä½™)],     # åˆ—2
    [P(å†—ä½™), P(éå†—ä½™)],     # åˆ—3
    ...
]

# ä¾‹å¦‚ï¼š
probabilities = [
    [0.85, 0.15],  # åˆ—1: 85%æ¦‚ç‡æ˜¯å†—ä½™åˆ—
    [0.23, 0.77],  # åˆ—2: 77%æ¦‚ç‡æ˜¯éå†—ä½™åˆ—  
    [0.92, 0.08],  # åˆ—3: 92%æ¦‚ç‡æ˜¯å†—ä½™åˆ—
]
```

å†³ç­–é€»è¾‘
```python
# åŸºäºæ¦‚ç‡è¿›è¡Œå†³ç­–
for i, prob in enumerate(probabilities):
    redundant_prob = prob[0]  # å†—ä½™æ¦‚ç‡
    non_redundant_prob = prob[1]  # éå†—ä½™æ¦‚ç‡
    
    if redundant_prob > THRESHOLD:  # ä¾‹å¦‚ 0.95
        decision = "å‰ªæè¿™ä¸€åˆ—"
    else:
        decision = "ä¿ç•™è¿™ä¸€åˆ—"
    
    print(f"åˆ—{i}: å†—ä½™æ¦‚ç‡={redundant_prob:.3f}, å†³ç­–={decision}")
```

ğŸ¯ åœ¨CGç®—æ³•ä¸­çš„åº”ç”¨
 é›†æˆåˆ°CGç®—æ³•ä¸­
```python
# åœ¨ CG_x_sn_rule_with_EGCN.py ä¸­çš„åº”ç”¨
def egcn_redundancy_check(column_features):
    """ä½¿ç”¨EGCN+Softmaxè¿›è¡Œå†—ä½™æ€§æ£€æŸ¥"""
    
    # 1. å‡†å¤‡è¾“å…¥æ•°æ®
    features = torch.tensor(column_features, dtype=torch.float32)
    adj_matrices = build_adjacency_matrices(features)
    
    # 2. EGCNå‰å‘ä¼ æ’­
    with torch.no_grad():
        node_embeddings, _ = egcn_model(adj_matrices, features)
        
        # 3. Softmaxåˆ†ç±»
        logits = egcn_classifier(node_embeddings)
        probabilities = F.softmax(logits, dim=1)
    
    # 4. åŸºäºé˜ˆå€¼å†³ç­–
    redundant_probs = probabilities[:, 0]  # å†—ä½™æ¦‚ç‡
    is_redundant = redundant_probs > REDUNDANCY_THRESHOLD
    
    return is_redundant, probabilities

# åœ¨CGä¸»å¾ªç¯ä¸­ä½¿ç”¨
for supplier in suppliers:
    # ç”Ÿæˆæ–°åˆ—
    new_column_features = solve_pricing_problem(supplier)
    
    # EGCN+Softmaxå†—ä½™æ€§æ£€æŸ¥
    is_redundant, probs = egcn_redundancy_check(new_column_features)
    
    if is_redundant:
        print(f"åˆ—è¢«å‰ªæï¼Œå†—ä½™æ¦‚ç‡: {probs[0]:.3f}")
        continue  # è·³è¿‡è¿™ä¸€åˆ—
    else:
        print(f"åˆ—è¢«ä¿ç•™ï¼Œéå†—ä½™æ¦‚ç‡: {probs[1]:.3f}")
        add_column_to_master_problem(new_column_features)
```

ğŸ”§ è®­ç»ƒè¿‡ç¨‹ä¸­çš„Softmax

æŸå¤±å‡½æ•°
```python
# ä½¿ç”¨äº¤å‰ç†µæŸå¤±è®­ç»ƒSoftmaxåˆ†ç±»å™¨
def train_epoch(self, train_loader):
    for batch in train_loader:
        features, adj_matrices, labels = batch
        
        # å‰å‘ä¼ æ’­
        node_embeddings, _ = self.model(adj_matrices, features)
        logits = self.classifier(node_embeddings)
        
        # è®¡ç®—æŸå¤±ï¼ˆè‡ªåŠ¨åŒ…å«Softmaxï¼‰
        loss = F.cross_entropy(logits, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        self.optimizer.step()
```

è¯„ä¼°æŒ‡æ ‡
```python
# è¯„ä¼°æ—¶è®¡ç®—å„ç§æŒ‡æ ‡
def evaluate(self, test_loader):
    all_predictions = []
    all_probabilities = []
    all_labels = []
    
    for batch in test_loader:
        features, adj_matrices, labels = batch
        
        with torch.no_grad():
            node_embeddings, _ = self.model(adj_matrices, features)
            logits = self.classifier(node_embeddings)
            
            # Softmaxæ¦‚ç‡
            probabilities = F.softmax(logits, dim=1)
            predictions = torch.argmax(probabilities, dim=1)
            
            all_predictions.extend(predictions.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—å‡†ç¡®ç‡ã€F1ç­‰æŒ‡æ ‡
    accuracy = accuracy_score(all_labels, all_predictions)
    f1 = f1_score(all_labels, all_predictions)
    
    return accuracy, f1, all_probabilities
```

âš™ï¸ è¶…å‚æ•°è°ƒä¼˜

åˆ†ç±»å™¨ç›¸å…³è¶…å‚æ•°
```python
# åœ¨ egcn_trainer.py ä¸­å¯è°ƒæ•´çš„å‚æ•°
classifier_config = {
    'hidden_dim': 16,        # éšè—å±‚ç»´åº¦
    'dropout_rate': 0.2,     # Dropoutæ¯”ç‡
    'num_classes': 2,        # åˆ†ç±»æ•°ï¼ˆå†—ä½™/éå†—ä½™ï¼‰
}

# è®­ç»ƒç›¸å…³è¶…å‚æ•°
training_config = {
    'learning_rate': 1e-3,   # å­¦ä¹ ç‡
    'weight_decay': 1e-5,    # æƒé‡è¡°å‡
    'redundancy_threshold': 0.95,  # å†—ä½™åˆ¤æ–­é˜ˆå€¼
}
```

é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æ
```python
# æµ‹è¯•ä¸åŒé˜ˆå€¼çš„å½±å“
thresholds = [0.90, 0.92, 0.94, 0.95, 0.96, 0.98]

for threshold in thresholds:
    REDUNDANCY_THRESHOLD = threshold
    
    # åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•
    pruning_rate = test_pruning_performance(val_dataset, threshold)
    
    print(f"é˜ˆå€¼ {threshold}: å‰ªæç‡ {pruning_rate:.2%}")
```

ğŸ“ˆ é¢„æœŸæ€§èƒ½

Softmaxè¾“å‡ºçš„å…¸å‹åˆ†å¸ƒ
```
è®­ç»ƒè‰¯å¥½çš„æ¨¡å‹åº”è¯¥äº§ç”Ÿï¼š

é«˜ç½®ä¿¡åº¦å†—ä½™åˆ—ï¼š
  [0.95, 0.05] - 95%ç¡®ä¿¡æ˜¯å†—ä½™çš„

é«˜ç½®ä¿¡åº¦éå†—ä½™åˆ—ï¼š
  [0.08, 0.92] - 92%ç¡®ä¿¡æ˜¯éå†—ä½™çš„

ä¸ç¡®å®šçš„åˆ—ï¼š
  [0.45, 0.55] - ä¸å¤ªç¡®å®š
```
