import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
import time
import os
from collections import defaultdict
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from egcn_o import EGCN
import utils as u

# å°è¯•å¯¼å…¥å‚æ•°æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
try:
    from MT_para_real import *
except ImportError:
    print("è­¦å‘Šï¼šæ— æ³•å¯¼å…¥MT_para_real.pyï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
    N_num = 5  # ç…¤ç‚­ç§ç±»æ•°
    T_num = 24  # æ—¶é—´æ­¥æ•°
    S_num = 10  # ä¾›åº”å•†æ•°


class ColumnDataset(Dataset):
    """
    åˆ—æ•°æ®é›†ç±»ï¼Œç”¨äºè®­ç»ƒEGCN
    """

    def __init__(self, columns_data, labels, max_columns_per_graph=50):
        """
        Args:
            columns_data: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå›¾çš„åˆ—ç‰¹å¾ [(features, adjacency), ...]
            labels: åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å¯¹åº”å›¾ä¸­æ¯åˆ—çš„æ ‡ç­¾
            max_columns_per_graph: æ¯ä¸ªå›¾æœ€å¤§åˆ—æ•°ï¼ˆç”¨äºpaddingï¼‰
        """
        self.columns_data = columns_data
        self.labels = labels
        self.max_columns = max_columns_per_graph

    def __len__(self):
        return len(self.columns_data)

    def __getitem__(self, idx):
        features, adj_matrices = self.columns_data[idx]
        labels = self.labels[idx]

        # Paddingåˆ°å›ºå®šå¤§å°
        n_cols = features.shape[0]
        if n_cols < self.max_columns:
            # Padding features
            pad_features = torch.zeros(self.max_columns - n_cols, features.shape[1])
            features = torch.cat([features, pad_features], dim=0)

            # Padding adjacency matrices
            padded_adj = {}
            for rel_type, adj in adj_matrices.items():
                pad_adj = torch.zeros(self.max_columns, self.max_columns)
                pad_adj[:n_cols, :n_cols] = adj
                padded_adj[rel_type] = pad_adj
            adj_matrices = padded_adj

            # Padding labels
            pad_labels = torch.full((self.max_columns - n_cols,), -1, dtype=torch.long)  # -1è¡¨ç¤ºpadding
            labels = torch.cat([labels, pad_labels], dim=0)

        return features, adj_matrices, labels, n_cols


class EGCNTrainer:
    """
    EGCNè®­ç»ƒå™¨
    """

    def __init__(self, device='cuda', learning_rate=1e-3, weight_decay=1e-5):
        """
        Args:
            device: è®¡ç®—è®¾å¤‡
            learning_rate: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
        """
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self.learning_rate = learning_rate
        self.weight_decay = weight_decay

        # åˆå§‹åŒ–EGCNæ¨¡å‹
        self.init_model()

        # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        self.criterion = nn.CrossEntropyLoss(ignore_index=-1)  # å¿½ç•¥paddingæ ‡ç­¾
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=learning_rate,
            weight_decay=weight_decay
        )
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', patience=10, factor=0.5
        )

        # è®­ç»ƒå†å²
        self.train_history = {
            'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []
        }
        self.val_history = {
            'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []
        }

        print(f"EGCNè®­ç»ƒå™¨å·²åˆå§‹åŒ–ï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def init_model(self):
        """åˆå§‹åŒ–EGCNæ¨¡å‹"""
        args = u.Namespace({
            'feats_per_node': N_num + T_num + 1,  # ç‰¹å¾ç»´åº¦
            'layer_1_feats': 64,  # éšè—å±‚ç»´åº¦
            'layer_2_feats': 32  # è¾“å‡ºåµŒå…¥ç»´åº¦
        })

        self.model = EGCN(args, device=self.device)
        self.model.to(self.device)

        # æ·»åŠ åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(args.layer_2_feats, 16),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(16, 2)  # äºŒåˆ†ç±»ï¼š0=å†—ä½™ï¼Œ1=éå†—ä½™
        ).to(self.device)

        # å°†åˆ†ç±»å™¨å‚æ•°æ·»åŠ åˆ°ä¼˜åŒ–å™¨
        all_params = list(self.model.parameters()) + list(self.classifier.parameters())
        self.optimizer = optim.Adam(all_params, lr=self.learning_rate, weight_decay=self.weight_decay)

    def load_real_cg_column_data(self, data_file='PP+ML/cg_real_column_data.pkl'):
        """
        åŠ è½½åŸºäºçœŸå®CGåˆ—ç”Ÿæˆè¿‡ç¨‹çš„è®­ç»ƒæ•°æ®

        Args:
            data_file: çœŸå®CGåˆ—æ•°æ®æ–‡ä»¶è·¯å¾„

        Returns:
            train_dataset, val_dataset æˆ– Noneå¦‚æœåŠ è½½å¤±è´¥
        """
        print("æ­£åœ¨åŠ è½½çœŸå®CGåˆ—æ•°æ®...")

        if not os.path.exists(data_file):
            print(f"âŒ çœŸå®CGåˆ—æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            print("è¯·å…ˆè¿è¡Œ cg_column_collector.py ç”ŸæˆçœŸå®åˆ—æ•°æ®")
            return None, None

        try:
            import pickle
            with open(data_file, 'rb') as f:
                cg_data = pickle.load(f)

            print(f"âœ… æˆåŠŸåŠ è½½çœŸå®CGåˆ—æ•°æ®:")
            print(f"  æ€»åˆ—æ•°: {cg_data['statistics']['total_columns']}")
            print(f"  éå†—ä½™åˆ—: {cg_data['statistics']['total_accepted']} (æ ‡ç­¾=1)")
            print(f"  å†—ä½™åˆ—: {cg_data['statistics']['total_rejected']} (æ ‡ç­¾=0)")
            print(f"  æ•°æ®æ¥æº: {cg_data['statistics']['seeds_processed']} ä¸ªéšæœºç§å­")

            # è½¬æ¢ä¸ºEGCNè®­ç»ƒæ ¼å¼
            features = np.array(cg_data['all_features'])
            labels = np.array(cg_data['all_labels'])
            suppliers = np.array(cg_data['all_suppliers'])
            iterations = np.array(cg_data['all_iterations'])

            print(f"  ç‰¹å¾ç»´åº¦: {features.shape[1]}")

            # æŒ‰è¿­ä»£åˆ†ç»„æ„å»ºå›¾æ•°æ®
            unique_seeds = cg_data['collection_info']['successful_seeds']

            # ä¸ºæ¯ä¸ªç§å­çš„æ¯ä¸ªè¿­ä»£åˆ›å»ºä¸€ä¸ªå›¾
            all_graphs = []
            all_graph_labels = []

            # æŒ‰ç§å­å’Œè¿­ä»£åˆ†ç»„
            seed_iter_groups = {}
            for i, (supplier, iteration) in enumerate(zip(suppliers, iterations)):
                # å‡è®¾ç§å­ä¿¡æ¯å­˜å‚¨åœ¨æŸå¤„ï¼Œè¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†
                seed_idx = i // 50  # å‡è®¾æ¯ä¸ªç§å­å¤§çº¦æœ‰50åˆ—
                if seed_idx >= len(unique_seeds):
                    seed_idx = len(unique_seeds) - 1

                key = (unique_seeds[seed_idx], iteration)
                if key not in seed_iter_groups:
                    seed_iter_groups[key] = []
                seed_iter_groups[key].append(i)

            print(f"æ„å»ºå›¾æ•°æ®ä¸­... (å…± {len(seed_iter_groups)} ä¸ªå›¾)")

            for (seed, iteration), indices in seed_iter_groups.items():
                if len(indices) < 2:  # è·³è¿‡åªæœ‰ä¸€åˆ—çš„å›¾
                    continue

                # æå–è¿™ä¸ªå›¾çš„ç‰¹å¾å’Œæ ‡ç­¾
                graph_features = torch.tensor(features[indices], dtype=torch.float32)
                graph_labels = torch.tensor(labels[indices], dtype=torch.long)

                # æ„å»ºé‚»æ¥çŸ©é˜µ
                n_nodes = len(indices)
                adj_matrices = {}

                # åŸºäºç‰¹å¾ç›¸ä¼¼åº¦æ„å»ºå¤šå…³ç³»é‚»æ¥çŸ©é˜µ
                # å‡è®¾ç‰¹å¾æ ¼å¼ï¼š[coal_quantities(N_num), time_features(T_num), vessel_count(1)]
                N_num = 5  # ç…¤ç‚­ç§ç±»æ•°
                T_num = 24  # æ—¶é—´æ­¥æ•°

                # ç…¤ç‚­å…³ç³»çŸ©é˜µ
                coal_adj = torch.zeros(n_nodes, n_nodes)
                # æ—¶é—´å…³ç³»çŸ©é˜µ
                time_adj = torch.zeros(n_nodes, n_nodes)
                # èˆ¹åªå…³ç³»çŸ©é˜µ
                vessel_adj = torch.zeros(n_nodes, n_nodes)

                for i in range(n_nodes):
                    for j in range(i + 1, n_nodes):
                        # ç…¤ç‚­ç›¸ä¼¼åº¦
                        coal_feat_i = graph_features[i, :N_num]
                        coal_feat_j = graph_features[j, :N_num]
                        coal_sim = 1.0 / (1.0 + torch.norm(coal_feat_i - coal_feat_j).item())

                        # æ—¶é—´ç›¸ä¼¼åº¦
                        time_feat_i = graph_features[i, N_num:N_num + T_num]
                        time_feat_j = graph_features[j, N_num:N_num + T_num]
                        time_sim = 1.0 / (1.0 + torch.norm(time_feat_i - time_feat_j).item())

                        # èˆ¹åªç›¸ä¼¼åº¦
                        vessel_feat_i = graph_features[i, -1]
                        vessel_feat_j = graph_features[j, -1]
                        vessel_sim = 1.0 / (1.0 + abs(vessel_feat_i - vessel_feat_j).item())

                        # è®¾ç½®è¾¹ï¼ˆç›¸ä¼¼åº¦é˜ˆå€¼ï¼‰
                        if coal_sim > 0.7:
                            coal_adj[i, j] = coal_adj[j, i] = 1.0
                        if time_sim > 0.7:
                            time_adj[i, j] = time_adj[j, i] = 1.0
                        if vessel_sim > 0.8:
                            vessel_adj[i, j] = vessel_adj[j, i] = 1.0

                adj_matrices = {
                    'coal': coal_adj,
                    'time': time_adj,
                    'vessel': vessel_adj
                }

                all_graphs.append((graph_features, adj_matrices))
                all_graph_labels.append(graph_labels)

            print(f"âœ… æ„å»ºäº† {len(all_graphs)} ä¸ªå›¾")

            # ä¸¥æ ¼çš„ä¸‰åˆ†å‰²ï¼šè®­ç»ƒé›†(60%) / éªŒè¯é›†(20%) / æµ‹è¯•é›†(20%)
            total_graphs = len(all_graphs)
            train_split_idx = int(0.6 * total_graphs)
            val_split_idx = int(0.8 * total_graphs)

            # éšæœºæ‰“ä¹±æ•°æ®ä»¥ç¡®ä¿åˆ†å‰²çš„éšæœºæ€§
            indices = np.random.permutation(total_graphs)

            train_indices = indices[:train_split_idx]
            val_indices = indices[train_split_idx:val_split_idx]
            test_indices = indices[val_split_idx:]

            train_data = [all_graphs[i] for i in train_indices]
            train_labels = [all_graph_labels[i] for i in train_indices]

            val_data = [all_graphs[i] for i in val_indices]
            val_labels = [all_graph_labels[i] for i in val_indices]

            test_data = [all_graphs[i] for i in test_indices]
            test_labels = [all_graph_labels[i] for i in test_indices]

            train_dataset = ColumnDataset(train_data, train_labels)
            val_dataset = ColumnDataset(val_data, val_labels)
            test_dataset = ColumnDataset(test_data, test_labels)

            print(f"ä¸¥æ ¼æ•°æ®åˆ†å‰²å®Œæˆï¼š")
            print(f"  è®­ç»ƒé›†: {len(train_dataset)} ä¸ªå›¾ ({len(train_dataset) / total_graphs:.1%})")
            print(f"  éªŒè¯é›†: {len(val_dataset)} ä¸ªå›¾ ({len(val_dataset) / total_graphs:.1%})")
            print(f"  æµ‹è¯•é›†: {len(test_dataset)} ä¸ªå›¾ ({len(test_dataset) / total_graphs:.1%})")

            return train_dataset, val_dataset, test_dataset

        except Exception as e:
            print(f"âŒ åŠ è½½çœŸå®CGåˆ—æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None

    def generate_training_data(self, num_graphs=1000, columns_per_graph_range=(10, 50)):
        """
        ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆæ¨¡æ‹Ÿæ•°æ®ï¼Œä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰

        Args:
            num_graphs: ç”Ÿæˆçš„å›¾æ•°é‡
            columns_per_graph_range: æ¯ä¸ªå›¾çš„åˆ—æ•°èŒƒå›´

        Returns:
            train_dataset, val_dataset
        """
        print("æ­£åœ¨ç”Ÿæˆæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®...")

        all_columns_data = []
        all_labels = []

        for graph_idx in range(num_graphs):
            if graph_idx % 100 == 0:
                print(f"ç”Ÿæˆè¿›åº¦: {graph_idx}/{num_graphs}")

            # éšæœºç¡®å®šè¿™ä¸ªå›¾çš„åˆ—æ•°
            n_columns = np.random.randint(*columns_per_graph_range)

            # ç”Ÿæˆåˆ—ç‰¹å¾
            features = self.generate_column_features(n_columns)

            # ç”Ÿæˆé‚»æ¥çŸ©é˜µ
            adj_matrices = self.generate_adjacency_matrices(features)

            # ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäºç›¸ä¼¼åº¦çš„å¯å‘å¼è§„åˆ™ï¼‰
            labels = self.generate_labels(features)

            all_columns_data.append((features, adj_matrices))
            all_labels.append(labels)

        # ä¸¥æ ¼çš„ä¸‰åˆ†å‰²ï¼šè®­ç»ƒé›†(60%) / éªŒè¯é›†(20%) / æµ‹è¯•é›†(20%)
        train_split_idx = int(0.6 * num_graphs)
        val_split_idx = int(0.8 * num_graphs)

        # éšæœºæ‰“ä¹±æ•°æ®
        indices = np.random.permutation(num_graphs)

        train_indices = indices[:train_split_idx]
        val_indices = indices[train_split_idx:val_split_idx]
        test_indices = indices[val_split_idx:]

        train_data = [all_columns_data[i] for i in train_indices]
        train_labels = [all_labels[i] for i in train_indices]

        val_data = [all_columns_data[i] for i in val_indices]
        val_labels = [all_labels[i] for i in val_indices]

        test_data = [all_columns_data[i] for i in test_indices]
        test_labels = [all_labels[i] for i in test_indices]

        train_dataset = ColumnDataset(train_data, train_labels)
        val_dataset = ColumnDataset(val_data, val_labels)
        test_dataset = ColumnDataset(test_data, test_labels)

        print(f"æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆï¼š")
        print(f"  è®­ç»ƒé›†: {len(train_dataset)} ä¸ªå›¾ ({len(train_dataset) / num_graphs:.1%})")
        print(f"  éªŒè¯é›†: {len(val_dataset)} ä¸ªå›¾ ({len(val_dataset) / num_graphs:.1%})")
        print(f"  æµ‹è¯•é›†: {len(test_dataset)} ä¸ªå›¾ ({len(test_dataset) / num_graphs:.1%})")

        return train_dataset, val_dataset, test_dataset

    def generate_column_features(self, n_columns):
        """
        ç”Ÿæˆåˆ—ç‰¹å¾çŸ©é˜µ

        Args:
            n_columns: åˆ—æ•°

        Returns:
            features: (n_columns, feature_dim) ç‰¹å¾çŸ©é˜µ
        """
        feature_dim = N_num + T_num + 1
        features = torch.zeros(n_columns, feature_dim)

        for i in range(n_columns):
            # ç…¤ç‚­æ•°é‡ç‰¹å¾ï¼ˆå‰N_numç»´ï¼‰
            coal_features = torch.rand(N_num) * 100  # 0-100çš„éšæœºé‡‡è´­é‡

            # æ—¶é—´ç‰¹å¾ï¼ˆä¸­é—´T_numç»´ï¼‰
            time_features = torch.rand(T_num) * 5  # 0-5çš„éšæœºæ³Šä½å ç”¨

            # èˆ¹åªç‰¹å¾ï¼ˆæœ€å1ç»´ï¼‰
            vessel_feature = torch.randint(1, 6, (1,)).float()  # 1-5è‰˜èˆ¹

            features[i] = torch.cat([coal_features, time_features, vessel_feature])

        return features

    def generate_adjacency_matrices(self, features):
        """
        ç”Ÿæˆé‚»æ¥çŸ©é˜µ

        Args:
            features: ç‰¹å¾çŸ©é˜µ

        Returns:
            adj_matrices: é‚»æ¥çŸ©é˜µå­—å…¸
        """
        n_columns = features.shape[0]
        relation_types = ['coal', 'time', 'vessel']
        adj_matrices = {}

        for rel_type in relation_types:
            adj = torch.zeros(n_columns, n_columns)

            for i in range(n_columns):
                for j in range(i + 1, n_columns):
                    # è®¡ç®—ç›¸ä¼¼åº¦
                    if rel_type == 'coal':
                        sim = self.calculate_similarity(
                            features[i, :N_num], features[j, :N_num]
                        )
                    elif rel_type == 'time':
                        sim = self.calculate_similarity(
                            features[i, N_num:N_num + T_num], features[j, N_num:N_num + T_num]
                        )
                    else:  # vessel
                        sim = 1.0 / (1.0 + abs(features[i, -1] - features[j, -1]))

                    # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œåˆ›å»ºè¾¹
                    if sim > 0.8:
                        adj[i, j] = adj[j, i] = 1.0

            # å½’ä¸€åŒ–
            degree = adj.sum(dim=1)
            degree[degree == 0] = 1  # é¿å…é™¤é›¶
            D_inv_sqrt = torch.diag(torch.pow(degree, -0.5))
            adj = torch.mm(torch.mm(D_inv_sqrt, adj), D_inv_sqrt)

            adj_matrices[rel_type] = adj

        return adj_matrices

    def calculate_similarity(self, vec1, vec2):
        """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ç›¸ä¼¼åº¦"""
        distance = torch.norm(vec1 - vec2).item()
        return 1.0 / (1.0 + distance)

    def generate_labels(self, features):
        """
        åŸºäºå¯å‘å¼è§„åˆ™ç”Ÿæˆæ ‡ç­¾

        Args:
            features: ç‰¹å¾çŸ©é˜µ

        Returns:
            labels: æ ‡ç­¾å‘é‡ï¼ˆ0=å†—ä½™ï¼Œ1=éå†—ä½™ï¼‰
        """
        n_columns = features.shape[0]
        labels = torch.ones(n_columns, dtype=torch.long)  # é»˜è®¤ä¸ºéå†—ä½™

        # æ‰¾åˆ°é«˜åº¦ç›¸ä¼¼çš„åˆ—å¯¹
        for i in range(n_columns):
            for j in range(i + 1, n_columns):
                # è®¡ç®—æ€»ä½“ç›¸ä¼¼åº¦
                coal_sim = self.calculate_similarity(
                    features[i, :N_num], features[j, :N_num]
                )
                time_sim = self.calculate_similarity(
                    features[i, N_num:N_num + T_num], features[j, N_num:N_num + T_num]
                )
                vessel_sim = 1.0 / (1.0 + abs(features[i, -1] - features[j, -1]))

                avg_sim = (coal_sim + time_sim + vessel_sim) / 3.0

                # å¦‚æœç›¸ä¼¼åº¦å¾ˆé«˜ï¼Œæ ‡è®°è¾ƒæ™šç”Ÿæˆçš„åˆ—ä¸ºå†—ä½™
                if avg_sim > 0.95:
                    labels[j] = 0  # å†—ä½™

        return labels

    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        self.classifier.train()

        total_loss = 0
        all_predictions = []
        all_labels = []

        for batch_idx, (features, adj_matrices, labels, n_cols_list) in enumerate(train_loader):
            features = features.to(self.device)
            labels = labels.to(self.device)

            # å°†é‚»æ¥çŸ©é˜µç§»åˆ°è®¾å¤‡ä¸Š
            adj_matrices_device = {}
            for rel_type in adj_matrices:
                adj_matrices_device[rel_type] = adj_matrices[rel_type].to(self.device)

            self.optimizer.zero_grad()

            batch_loss = 0
            batch_predictions = []
            batch_true_labels = []

            # å¯¹batchä¸­çš„æ¯ä¸ªå›¾åˆ†åˆ«å¤„ç†
            for i in range(features.shape[0]):
                n_cols = n_cols_list[i].item()
                if n_cols <= 1:  # è·³è¿‡åªæœ‰ä¸€åˆ—æˆ–æ²¡æœ‰åˆ—çš„å›¾
                    continue

                # æå–å½“å‰å›¾çš„æ•°æ®
                graph_features = features[i, :n_cols]
                graph_labels = labels[i, :n_cols]
                graph_adj = {
                    rel_type: adj_matrices_device[rel_type][i, :n_cols, :n_cols]
                    for rel_type in adj_matrices_device
                }

                # æ¨¡æ‹Ÿæ–°åˆ—çš„èšåˆç‰¹å¾
                X_new = torch.mean(graph_features, dim=0)

                # EGCNå‰å‘ä¼ æ’­
                node_embeddings, _ = self.model.forward(
                    node_features=graph_features,
                    X_new=X_new,
                    existing_embeddings=None,
                    update_weights=True
                )

                # åˆ†ç±»
                logits = self.classifier(node_embeddings)

                # è®¡ç®—æŸå¤±
                loss = self.criterion(logits, graph_labels)
                batch_loss += loss

                # æ”¶é›†é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾
                with torch.no_grad():
                    predictions = torch.argmax(logits, dim=1)
                    batch_predictions.extend(predictions.cpu().numpy())
                    batch_true_labels.extend(graph_labels.cpu().numpy())

            if batch_loss > 0:
                batch_loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(self.model.parameters()) + list(self.classifier.parameters()),
                    max_norm=1.0
                )
                self.optimizer.step()

                total_loss += batch_loss.item()
                all_predictions.extend(batch_predictions)
                all_labels.extend(batch_true_labels)

        # è®¡ç®—æŒ‡æ ‡
        if len(all_predictions) > 0:
            accuracy = accuracy_score(all_labels, all_predictions)
            precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)
            recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)
            f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)
        else:
            accuracy = precision = recall = f1 = 0.0

        avg_loss = total_loss / len(train_loader) if len(train_loader) > 0 else 0.0

        return avg_loss, accuracy, precision, recall, f1

    def validate_epoch(self, val_loader):
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        self.classifier.eval()

        total_loss = 0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for features, adj_matrices, labels, n_cols_list in val_loader:
                features = features.to(self.device)
                labels = labels.to(self.device)

                adj_matrices_device = {}
                for rel_type in adj_matrices:
                    adj_matrices_device[rel_type] = adj_matrices[rel_type].to(self.device)

                batch_loss = 0
                batch_predictions = []
                batch_true_labels = []

                for i in range(features.shape[0]):
                    n_cols = n_cols_list[i].item()
                    if n_cols <= 1:
                        continue

                    graph_features = features[i, :n_cols]
                    graph_labels = labels[i, :n_cols]
                    graph_adj = {
                        rel_type: adj_matrices_device[rel_type][i, :n_cols, :n_cols]
                        for rel_type in adj_matrices_device
                    }

                    X_new = torch.mean(graph_features, dim=0)

                    node_embeddings, _ = self.model.forward(
                        node_features=graph_features,
                        X_new=X_new,
                        existing_embeddings=None,
                        update_weights=True
                    )

                    logits = self.classifier(node_embeddings)
                    loss = self.criterion(logits, graph_labels)
                    batch_loss += loss

                    predictions = torch.argmax(logits, dim=1)
                    batch_predictions.extend(predictions.cpu().numpy())
                    batch_true_labels.extend(graph_labels.cpu().numpy())

                total_loss += batch_loss.item()
                all_predictions.extend(batch_predictions)
                all_labels.extend(batch_true_labels)

        if len(all_predictions) > 0:
            accuracy = accuracy_score(all_labels, all_predictions)
            precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)
            recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)
            f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)
        else:
            accuracy = precision = recall = f1 = 0.0

        avg_loss = total_loss / len(val_loader) if len(val_loader) > 0 else 0.0

        return avg_loss, accuracy, precision, recall, f1

    def final_test_evaluation(self, test_dataset, model_path=None):
        """
        åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°ï¼ˆä¸¥æ ¼éªŒè¯ï¼‰

        Args:
            test_dataset: ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®é›†
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œç”¨äºåŠ è½½æœ€ä½³æ¨¡å‹ï¼‰

        Returns:
            test_results: è¯¦ç»†çš„æµ‹è¯•ç»“æœ
        """
        print("=" * 60)
        print("ğŸ”¬ å¼€å§‹åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°")
        print("=" * 60)

        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼ŒåŠ è½½æœ€ä½³æ¨¡å‹
        if model_path and os.path.exists(model_path):
            print(f"ğŸ“‚ åŠ è½½æœ€ä½³æ¨¡å‹: {model_path}")
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.classifier.load_state_dict(checkpoint['classifier_state_dict'])
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè®­ç»ƒè½®æ¬¡: {checkpoint.get('epoch', 'unknown')}ï¼‰")

        self.model.eval()
        self.classifier.eval()

        test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)

        all_predictions = []
        all_probabilities = []
        all_true_labels = []
        all_losses = []

        print(f"ğŸ“Š æµ‹è¯•é›†è§„æ¨¡: {len(test_dataset)} ä¸ªå›¾")

        with torch.no_grad():
            for batch_idx, (features, adj_matrices, labels) in enumerate(test_loader):
                if batch_idx % 10 == 0:
                    print(f"  å¤„ç†è¿›åº¦: {batch_idx + 1}/{len(test_loader)}")

                features = features.to(self.device)
                labels = labels.to(self.device)

                # è½¬æ¢é‚»æ¥çŸ©é˜µ
                adj_dict = {}
                for relation in ['coal', 'time', 'vessel']:
                    adj_dict[relation] = adj_matrices[relation].to(self.device)

                try:
                    # å‰å‘ä¼ æ’­
                    node_embeddings, _ = self.model(adj_dict, features)
                    logits = self.classifier(node_embeddings)

                    # è®¡ç®—æŸå¤±
                    loss = F.cross_entropy(logits, labels)
                    all_losses.append(loss.item())

                    # è·å–é¢„æµ‹
                    probabilities = F.softmax(logits, dim=1)
                    predictions = torch.argmax(probabilities, dim=1)

                    all_predictions.extend(predictions.cpu().numpy())
                    all_probabilities.extend(probabilities.cpu().numpy())
                    all_true_labels.extend(labels.cpu().numpy())

                except Exception as e:
                    print(f"âš ï¸ æ‰¹æ¬¡ {batch_idx} å¤„ç†å¤±è´¥: {e}")
                    continue

        # è®¡ç®—è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
        all_predictions = np.array(all_predictions)
        all_probabilities = np.array(all_probabilities)
        all_true_labels = np.array(all_true_labels)

        # åŸºæœ¬æŒ‡æ ‡
        test_loss = np.mean(all_losses)
        accuracy = accuracy_score(all_true_labels, all_predictions)
        precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=0)
        recall = recall_score(all_true_labels, all_predictions, average='weighted', zero_division=0)
        f1 = f1_score(all_true_labels, all_predictions, average='weighted', zero_division=0)

        # æ··æ·†çŸ©é˜µ
        from sklearn.metrics import confusion_matrix, classification_report
        cm = confusion_matrix(all_true_labels, all_predictions)

        # AUC-ROCï¼ˆå¦‚æœæ˜¯äºŒåˆ†ç±»ï¼‰
        auc_roc = None
        if len(np.unique(all_true_labels)) == 2:
            from sklearn.metrics import roc_auc_score
            auc_roc = roc_auc_score(all_true_labels, all_probabilities[:, 1])

        # æŒ‰ç±»åˆ«ç»Ÿè®¡
        unique_labels = np.unique(all_true_labels)
        class_stats = {}
        for label in unique_labels:
            mask = all_true_labels == label
            class_stats[label] = {
                'count': np.sum(mask),
                'accuracy': np.sum(all_predictions[mask] == label) / np.sum(mask),
                'avg_confidence': np.mean(np.max(all_probabilities[mask], axis=1))
            }

        # æ•´ç†æµ‹è¯•ç»“æœ
        test_results = {
            'test_loss': test_loss,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_roc': auc_roc,
            'confusion_matrix': cm,
            'class_statistics': class_stats,
            'num_samples': len(all_true_labels),
            'predictions': all_predictions,
            'probabilities': all_probabilities,
            'true_labels': all_true_labels
        }

        # æ‰“å°è¯¦ç»†ç»“æœ
        print("\\nğŸ“‹ æœ€ç»ˆæµ‹è¯•ç»“æœï¼ˆç‹¬ç«‹æµ‹è¯•é›†ï¼‰")
        print("=" * 50)
        print(f"æµ‹è¯•æŸå¤±:     {test_loss:.4f}")
        print(f"å‡†ç¡®ç‡:       {accuracy:.4f} ({accuracy * 100:.2f}%)")
        print(f"ç²¾ç¡®ç‡:       {precision:.4f}")
        print(f"å¬å›ç‡:       {recall:.4f}")
        print(f"F1åˆ†æ•°:       {f1:.4f}")
        if auc_roc:
            print(f"AUC-ROC:      {auc_roc:.4f}")

        print(f"\\nğŸ“Š æ··æ·†çŸ©é˜µ:")
        print("    é¢„æµ‹")
        print("      0    1")
        print(f"çœŸ 0 {cm[0, 0]:4d} {cm[0, 1]:4d}")
        print(f"å® 1 {cm[1, 0]:4d} {cm[1, 1]:4d}")

        print(f"\\nğŸ” æŒ‰ç±»åˆ«ç»Ÿè®¡:")
        for label, stats in class_stats.items():
            label_name = "å†—ä½™åˆ—" if label == 0 else "éå†—ä½™åˆ—"
            print(f"  {label_name} (æ ‡ç­¾={label}):")
            print(f"    æ ·æœ¬æ•°: {stats['count']}")
            print(f"    å‡†ç¡®ç‡: {stats['accuracy']:.4f}")
            print(f"    å¹³å‡ç½®ä¿¡åº¦: {stats['avg_confidence']:.4f}")

        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = f"PP+ML/final_test_results_{int(time.time())}.pkl"
        import pickle
        with open(results_file, 'wb') as f:
            pickle.dump(test_results, f)

        print(f"\\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

        # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
        print(f"\\nğŸ“ˆ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
        target_names = ['å†—ä½™åˆ—', 'éå†—ä½™åˆ—']
        print(classification_report(all_true_labels, all_predictions, target_names=target_names))

        return test_results

    def train(self, num_epochs=100, batch_size=16, save_path='egcn_model.pth', use_real_data=True):
        """
        å®Œæ•´çš„è®­ç»ƒæµç¨‹

        Args:
            num_epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹å¤§å°
            save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
            use_real_data: æ˜¯å¦ä½¿ç”¨çœŸå®CGæ•°æ®
        """
        print("=" * 60)
        print("å¼€å§‹EGCNæ¨¡å‹è®­ç»ƒ")
        print("=" * 60)

        # ä¼˜å…ˆå°è¯•åŠ è½½çœŸå®CGåˆ—æ•°æ®
        if use_real_data:
            train_dataset, val_dataset, test_dataset = self.load_real_cg_column_data()
            if train_dataset is None:
                print("âš ï¸ çœŸå®CGåˆ—æ•°æ®åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®")
                train_dataset, val_dataset, test_dataset = self.generate_training_data()
        else:
            train_dataset, val_dataset, test_dataset = self.generate_training_data()

        train_loader = DataLoader(
            train_dataset, batch_size=batch_size, shuffle=True, num_workers=0
        )
        val_loader = DataLoader(
            val_dataset, batch_size=batch_size, shuffle=False, num_workers=0
        )

        best_val_f1 = 0.0
        patience_counter = 0
        patience = 20

        print(f"è®­ç»ƒå‚æ•°:")
        print(f"  - è®­ç»ƒè½®æ•°: {num_epochs}")
        print(f"  - æ‰¹å¤§å°: {batch_size}")
        print(f"  - å­¦ä¹ ç‡: {self.learning_rate}")
        print(f"  - è®¾å¤‡: {self.device}")
        print()

        for epoch in range(num_epochs):
            start_time = time.time()

            # è®­ç»ƒ
            train_loss, train_acc, train_prec, train_rec, train_f1 = self.train_epoch(train_loader)

            # éªŒè¯
            val_loss, val_acc, val_prec, val_rec, val_f1 = self.validate_epoch(val_loader)

            # å­¦ä¹ ç‡è°ƒåº¦
            self.scheduler.step(val_loss)

            # è®°å½•å†å²
            self.train_history['loss'].append(train_loss)
            self.train_history['accuracy'].append(train_acc)
            self.train_history['precision'].append(train_prec)
            self.train_history['recall'].append(train_rec)
            self.train_history['f1'].append(train_f1)

            self.val_history['loss'].append(val_loss)
            self.val_history['accuracy'].append(val_acc)
            self.val_history['precision'].append(val_prec)
            self.val_history['recall'].append(val_rec)
            self.val_history['f1'].append(val_f1)

            epoch_time = time.time() - start_time

            # æ‰“å°è¿›åº¦
            if epoch % 5 == 0 or epoch == num_epochs - 1:
                print(f"Epoch [{epoch + 1}/{num_epochs}] ({epoch_time:.2f}s)")
                print(f"  Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}, F1={train_f1:.4f}")
                print(f"  Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}, F1={val_f1:.4f}")
                print(f"  LR: {self.optimizer.param_groups[0]['lr']:.6f}")
                print()

            # æ—©åœå’Œæ¨¡å‹ä¿å­˜
            if val_f1 > best_val_f1:
                best_val_f1 = val_f1
                patience_counter = 0

                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save({
                    'model_state_dict': self.model.state_dict(),
                    'classifier_state_dict': self.classifier.state_dict(),
                    'optimizer_state_dict': self.optimizer.state_dict(),
                    'epoch': epoch,
                    'best_val_f1': best_val_f1,
                    'train_history': self.train_history,
                    'val_history': self.val_history
                }, save_path)

                print(f"  âœ… æ–°çš„æœ€ä½³æ¨¡å‹å·²ä¿å­˜ (Val F1: {best_val_f1:.4f})")
            else:
                patience_counter += 1

            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                print(f"\næ—©åœè§¦å‘ï¼éªŒè¯F1å·²ç»{patience}è½®æ²¡æœ‰æ”¹å–„")
                break

        print("=" * 60)
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯F1: {best_val_f1:.4f}")
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        print("=" * 60)

        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves()

        # ğŸ”¬ ä¸¥æ ¼éªŒè¯ï¼šåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
        print("\\n" + "=" * 60)
        print("ğŸ”¬ å¼€å§‹ä¸¥æ ¼éªŒè¯ï¼šç‹¬ç«‹æµ‹è¯•é›†è¯„ä¼°")
        print("=" * 60)

        # ç¡®ä¿ä½¿ç”¨æœ€ä½³æ¨¡å‹è¿›è¡Œæµ‹è¯•
        test_results = self.final_test_evaluation(test_dataset, save_path)

        print("\\nğŸ¯ ä¸¥æ ¼éªŒè¯å®Œæˆï¼")

        return test_results

    def plot_training_curves(self):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # Lossæ›²çº¿
        axes[0, 0].plot(self.train_history['loss'], label='Train Loss')
        axes[0, 0].plot(self.val_history['loss'], label='Val Loss')
        axes[0, 0].set_title('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True)

        # Accuracyæ›²çº¿
        axes[0, 1].plot(self.train_history['accuracy'], label='Train Acc')
        axes[0, 1].plot(self.val_history['accuracy'], label='Val Acc')
        axes[0, 1].set_title('Accuracy')
        axes[0, 1].legend()
        axes[0, 1].grid(True)

        # Precisionæ›²çº¿
        axes[1, 0].plot(self.train_history['precision'], label='Train Prec')
        axes[1, 0].plot(self.val_history['precision'], label='Val Prec')
        axes[1, 0].set_title('Precision')
        axes[1, 0].legend()
        axes[1, 0].grid(True)

        # F1æ›²çº¿
        axes[1, 1].plot(self.train_history['f1'], label='Train F1')
        axes[1, 1].plot(self.val_history['f1'], label='Val F1')
        axes[1, 1].set_title('F1 Score')
        axes[1, 1].legend()
        axes[1, 1].grid(True)

        plt.tight_layout()
        plt.savefig('PP+ML/egcn_training_curves.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ°: PP+ML/egcn_training_curves.png")

    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        checkpoint = torch.load(model_path, map_location=self.device)

        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.classifier.load_state_dict(checkpoint['classifier_state_dict'])

        if 'train_history' in checkpoint:
            self.train_history = checkpoint['train_history']
            self.val_history = checkpoint['val_history']

        print(f"æ¨¡å‹å·²ä» {} åŠ è½½å®Œæˆ")
        print(f"æœ€ä½³éªŒè¯F1: {checkpoint.get('best_val_f1', 'N/A')}")

    def evaluate_on_real_data(self, columns_features, similarity_threshold=0.95):
        """
        åœ¨çœŸå®æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹

        Args:
            columns_features: çœŸå®çš„åˆ—ç‰¹å¾åˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼

        Returns:
            predictions, probabilities
        """
        self.model.eval()
        self.classifier.eval()

        if len(columns_features) <= 1:
            return [], []

        with torch.no_grad():
            # è½¬æ¢ä¸ºå¼ é‡
            features = torch.tensor(np.array(columns_features), dtype=torch.float32, device=self.device)

            # æ„å»ºé‚»æ¥çŸ©é˜µ
            adj_matrices = self.generate_adjacency_matrices(features)
            for rel_type in adj_matrices:
                adj_matrices[rel_type] = adj_matrices[rel_type].to(self.device)

            # è®¡ç®—èšåˆç‰¹å¾
            X_new = torch.mean(features, dim=0)

            # EGCNå‰å‘ä¼ æ’­
            node_embeddings, _ = self.model.forward(
                node_features=features,
                X_new=X_new,
                existing_embeddings=None,
                update_weights=True
            )

            # åˆ†ç±»
            logits = self.classifier(node_embeddings)
            probabilities = F.softmax(logits, dim=1)
            predictions = torch.argmax(logits, dim=1)

            return predictions.cpu().numpy(), probabilities.cpu().numpy()


def main():
    """è®­ç»ƒEGCNæ¨¡å‹çš„ä¸»å‡½æ•°"""
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = EGCNTrainer(
        device='cuda',
        learning_rate=1e-3,
        weight_decay=1e-5
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        num_epochs=100,
        batch_size=8,
        save_path='PP+ML/egcn_trained_model.pth'
    )

    # æµ‹è¯•æ¨¡å‹
    print("\næµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹...")

    # ç”Ÿæˆä¸€äº›æµ‹è¯•æ•°æ®
    test_features = [
        np.random.rand(N_num + T_num + 1) for _ in range(10)
    ]

    predictions, probabilities = trainer.evaluate_on_real_data(test_features)

    print("æµ‹è¯•ç»“æœ:")
    for i, (pred, prob) in enumerate(zip(predictions, probabilities)):
        redundant_prob = prob[0]
        non_redundant_prob = prob[1]
        print(f"åˆ— {i}: {'å†—ä½™' if pred == 0 else 'éå†—ä½™'} "
              f"(å†—ä½™æ¦‚ç‡: {redundant_prob:.3f}, éå†—ä½™æ¦‚ç‡: {non_redundant_prob:.3f})")


if __name__ == "__main__":
    main()
