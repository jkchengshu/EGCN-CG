import torch
from torch.nn.parameter import Parameter
import torch.nn as nn
import torch.nn.functional as F
import math


class EGCN(torch.nn.Module):
    """
    Implements the Evolving Graph Convolutional Network (EGCN).

    This model uses a multi-relational Graph Convolutional Network (GCN) to
    generate embeddings for columns in a column generation context. The key novelty
    is that the GCN's weight matrices are dynamically updated at each iteration
    using a Gated Recurrent Unit (GRU), allowing the model to adapt as new
    columns (information) are introduced.

    The EGCN implements the core algorithm described in the paper:
    1. Initialize multi-relational GCN weight matrices for coal, time, and vessel relations
    2. Use GRU cells to evolve these weight matrices based on new column features
    3. Apply multi-relational graph convolution with updated weights
    4. Generate high-quality node embeddings for downstream classification

    Args:
        args (Namespace): A namespace containing model hyperparameters:
            - feats_per_node (int): The dimensionality of the input node features.
            - layer_1_feats (int): The number of features in the hidden GCN layer.
            - layer_2_feats (int): The dimensionality of the final output embeddings.
        device (str): The device to run the model on ('cpu' or 'cuda'). Default: 'cpu'.
        
    Attributes:
        relation_types (list): List of relation types ['coal', 'time', 'vessel']
        gcn_weights (nn.ParameterDict): Learnable GCN weight matrices for each layer and relation
        weight_grus (nn.ModuleDict): GRU cells for evolving each weight matrix
        
    Example:
        >>> import argparse
        >>> args = argparse.Namespace(
        ...     feats_per_node=15,
        ...     layer_1_feats=64, 
        ...     layer_2_feats=32
        ... )
        >>> model = EGCN(args, device='cuda')
        >>> print(model)
    """
    
    def __init__(self, args, device='cpu'):
        super().__init__()
        self.device = device
        
        # Model dimensions
        self.node_feature_dim = args.feats_per_node  # Initial node features
        self.hidden_dim = args.layer_1_feats  # Hidden layer dimension (64 in paper)
        self.output_dim = args.layer_2_feats  # Output embedding dimension
        
        # Relation types as defined in paper
        self.relation_types = ['coal', 'time', 'vessel']
        self.num_relations = len(self.relation_types)
        
        # GCN layers (2 layers as mentioned in paper)
        self.num_layers = 2
        
        # Initialize GCN weight matrices for each relation and layer
        # These will be dynamically updated by GRU
        self.init_gcn_weights()
        
        # GRU for dynamic weight evolution
        self.init_weight_grus()
        
        self.activation = nn.ReLU()
        
    def init_gcn_weights(self):
        """
        Initialize GCN weight matrices for each layer and relation type.
        
        Creates learnable parameter matrices for each combination of:
        - Layer (0: input->hidden, 1: hidden->output)
        - Relation type (coal, time, vessel)
        
        Weight matrices are initialized with small random values to break symmetry.
        The naming convention is 'layer_{layer_idx}_{relation_type}'.
        """
        self.gcn_weights = nn.ParameterDict()
        
        # Layer 0: node_feature_dim -> hidden_dim
        for r in self.relation_types:
            self.gcn_weights[f'layer_0_{r}'] = Parameter(
                torch.randn(self.node_feature_dim, self.hidden_dim) * 0.1
            )
            
        # Layer 1: hidden_dim -> output_dim  
        for r in self.relation_types:
            self.gcn_weights[f'layer_1_{r}'] = Parameter(
                torch.randn(self.hidden_dim, self.output_dim) * 0.1
            )
    
    def init_weight_grus(self):
        """
        Initialize GRU modules for evolving each weight matrix.
        
        Creates a GRU cell for each GCN weight matrix. Each GRU:
        - Takes new column features as input (size: feats_per_node)
        - Maintains the flattened weight matrix as hidden state
        - Outputs evolved weight matrix for the next iteration
        
        This allows the model to adapt its graph convolution weights
        based on the characteristics of newly generated columns.
        """
        self.weight_grus = nn.ModuleDict()
        
        # For each layer and relation, create a GRU to evolve the weight matrix
        for layer_idx in range(self.num_layers):
            if layer_idx == 0:
                weight_size = self.node_feature_dim * self.hidden_dim
            else:
                weight_size = self.hidden_dim * self.output_dim
                
            for r in self.relation_types:
                gru_name = f'layer_{layer_idx}_{r}'
                # GRU input: aggregated new column features
                # GRU hidden state: flattened weight matrix
                self.weight_grus[gru_name] = nn.GRUCell(
                    input_size=self.node_feature_dim,  # X_i dimension
                    hidden_size=weight_size  # Flattened weight matrix size
                )
    
    
    def update_weights_with_gru(self, X_new):
        """
        Updates the GCN weight matrices using the GRU cells.

        Each GCN weight matrix (for each layer and relation) is treated as the
        hidden state of a corresponding GRU cell. The input to the GRU is the
        aggregated feature vector of the newly generated column(s).
        
        This mechanism allows the model to evolve its graph convolution weights
        dynamically as new information becomes available through the column
        generation process.

        Args:
            X_new (torch.Tensor): A tensor of shape (feats_per_node,)
                                  representing the new column's aggregated features.
                                  
        Note:
            This method modifies the model's gcn_weights in-place. The updated
            weights will be used in subsequent forward passes.
        """
        X_new = X_new.unsqueeze(0)  # (1, feature_dim) for GRU input
        
        for layer_idx in range(self.num_layers):
            for r in self.relation_types:
                gru_name = f'layer_{layer_idx}_{r}'
                weight_name = f'layer_{layer_idx}_{r}'
                
                # Get current weight matrix and flatten it
                current_weight = self.gcn_weights[weight_name]
                flattened_weight = current_weight.view(-1).unsqueeze(0)  # (1, weight_size)
                
                # Update weight using GRU
                new_flattened_weight = self.weight_grus[gru_name](X_new, flattened_weight)
                
                # Reshape back to original weight matrix shape
                new_weight = new_flattened_weight.view(current_weight.shape)
                
                # Update the parameter
                self.gcn_weights[weight_name].data = new_weight.squeeze(0)
    
    def gcn_forward(self, H, adjacency_matrices):
        """
        Performs multi-relational graph convolution forward pass.
        
        Applies graph convolution across multiple relation types (coal, time, vessel)
        and aggregates the results. Uses the current GCN weight matrices which
        may have been updated by the GRU mechanism.
        
        The computation for each layer follows: H^(l+1) = σ(∑_r A_r * H^(l) * W_r^(l))
        where:
        - A_r is the normalized adjacency matrix for relation r
        - H^(l) are the node features at layer l  
        - W_r^(l) is the weight matrix for relation r at layer l
        - σ is the activation function (ReLU for hidden layers)

        Args:
            H (torch.Tensor): Node feature matrix of shape (N, feature_dim)
            adjacency_matrices (dict): Dictionary mapping relation types to 
                normalized adjacency matrices, each of shape (N, N)
                
        Returns:
            torch.Tensor: Updated node embeddings of shape (N, output_dim)
        """
        for layer_idx in range(self.num_layers):
            H_next = torch.zeros(H.size(0), 
                               self.hidden_dim if layer_idx == 0 else self.output_dim,
                               device=self.device)
            
            # Aggregate over all relation types
            for r in self.relation_types:
                if r in adjacency_matrices:
                    A_r = adjacency_matrices[r]
                    W_r = self.gcn_weights[f'layer_{layer_idx}_{r}']
                    
                    # GCN propagation: A * H * W
                    H_r = torch.mm(torch.mm(A_r, H), W_r)
                    H_next += H_r
            
            # Apply activation (ReLU for hidden layers, identity for output layer)
            if layer_idx < self.num_layers - 1:
                H = self.activation(H_next)
            else:
                H = H_next
                
        return H
    
    
    def forward(self, node_features, adjacency_matrices, X_new=None):
        """
        EGCN forward pass - generates high-quality node embeddings.
        
        This method implements the core EGCN algorithm which dynamically evolves
        GCN weight matrices using GRU cells and applies multi-relational graph
        convolution to generate node embeddings.
        
        Args:
            node_features (torch.Tensor): Current node feature matrix of shape 
                (N, feature_dim) containing concatenated coal, time, and vessel features.
            adjacency_matrices (dict): Dictionary mapping relation types to their 
                normalized adjacency matrices. Expected keys: 'coal', 'time', 'vessel'.
                Each matrix should have shape (N, N).
            X_new (torch.Tensor, optional): Aggregated features of newly generated 
                columns with shape (feature_dim,). If provided, triggers GRU-based
                weight evolution. Defaults to None.
                
        Returns:
            torch.Tensor: Node embeddings of shape (N, output_dim) representing
                high-quality feature representations for downstream tasks.
                
        Example:
            >>> model = EGCN(args, device='cuda')
            >>> node_features = torch.randn(100, 15)  # 100 nodes, 15 features
            >>> adj_matrices = {...}  # Pre-computed adjacency matrices
            >>> embeddings = model(node_features, adj_matrices)
            >>> print(embeddings.shape)  # torch.Size([100, output_dim])
        """
        # Step 1: Update GCN weights if new columns are provided
        if X_new is not None:
            self.update_weights_with_gru(X_new)
        
        # Step 2: Apply multi-relational GCN to generate embeddings
        node_embeddings = self.gcn_forward(node_features, adjacency_matrices)
        
        return node_embeddings

