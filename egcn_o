import utils as u
import torch
from torch.nn.parameter import Parameter
import torch.nn as nn
import torch.nn.functional as F
import math


class EGCN(torch.nn.Module):
    """
    Evolving Graph Convolutional Network for Column Generation

    Based on the paper's Algorithm 5.3, this model:
    1. Constructs multi-relational graphs (coal, time, vessel relationships)
    2. Uses GRU to dynamically evolve GCN weight matrices over CG iterations
    3. Applies GCN for feature aggregation with context-aware embeddings
    4. Generates high-quality node embeddings for downstream classification

    Note: Classification is handled by external classifier for better modularity
    """

    def __init__(self, args, device='cpu'):
        super().__init__()
        self.device = device

        # Model dimensions
        self.node_feature_dim = args.feats_per_node  # Initial node features
        self.hidden_dim = args.layer_1_feats  # Hidden layer dimension (64 in paper)
        self.output_dim = args.layer_2_feats  # Output embedding dimension

        # Relation types as defined in paper
        self.relation_types = ['coal', 'time', 'vessel']
        self.num_relations = len(self.relation_types)

        # GCN layers (2 layers as mentioned in paper)
        self.num_layers = 2

        # Initialize GCN weight matrices for each relation and layer
        # These will be dynamically updated by GRU
        self.init_gcn_weights()

        # GRU for dynamic weight evolution
        self.init_weight_grus()

        self.activation = nn.ReLU()

    def init_gcn_weights(self):
        """Initialize GCN weight matrices for each layer and relation type"""
        self.gcn_weights = nn.ParameterDict()

        # Layer 0: node_feature_dim -> hidden_dim
        for r in self.relation_types:
            self.gcn_weights[f'layer_0_{r}'] = Parameter(
                torch.randn(self.node_feature_dim, self.hidden_dim) * 0.1
            )

        # Layer 1: hidden_dim -> output_dim
        for r in self.relation_types:
            self.gcn_weights[f'layer_1_{r}'] = Parameter(
                torch.randn(self.hidden_dim, self.output_dim) * 0.1
            )

    def init_weight_grus(self):
        """Initialize GRU modules for each weight matrix"""
        self.weight_grus = nn.ModuleDict()

        # For each layer and relation, create a GRU to evolve the weight matrix
        for layer_idx in range(self.num_layers):
            if layer_idx == 0:
                weight_size = self.node_feature_dim * self.hidden_dim
            else:
                weight_size = self.hidden_dim * self.output_dim

            for r in self.relation_types:
                gru_name = f'layer_{layer_idx}_{r}'
                # GRU input: aggregated new column features
                # GRU hidden state: flattened weight matrix
                self.weight_grus[gru_name] = nn.GRUCell(
                    input_size=self.node_feature_dim,  # X_i dimension
                    hidden_size=weight_size  # Flattened weight matrix size
                )

    def construct_adjacency_matrices(self, node_features, similarity_threshold=0.95):
        """
        Construct multi-relational adjacency matrices based on similarity

        Args:
            node_features: (N, feature_dim) node feature matrix
            similarity_threshold: threshold for edge creation (0.95 in paper)

        Returns:
            Dict of normalized adjacency matrices for each relation type
        """
        N = node_features.size(0)
        adjacency_matrices = {}

        for r_idx, r in enumerate(self.relation_types):
            # Create adjacency matrix for relation type r
            A_r = torch.zeros(N, N, device=self.device)

            # Compute similarity for specific feature dimensions
            # This is a simplified version - in practice, you'd implement
            # the specific similarity functions for coal, time, vessel
            if r == 'coal':
                # Similarity based on coal quantities (first part of features)
                feature_slice = node_features[:, :node_features.size(1) // 3]
            elif r == 'time':
                # Similarity based on time features
                feature_slice = node_features[:, node_features.size(1) // 3:2 * node_features.size(1) // 3]
            else:  # vessel
                # Similarity based on vessel features
                feature_slice = node_features[:, 2 * node_features.size(1) // 3:]

            # Compute cosine similarity
            feature_slice_norm = F.normalize(feature_slice, p=2, dim=1)
            similarity_matrix = torch.mm(feature_slice_norm, feature_slice_norm.t())

            # Create edges based on threshold
            A_r = (similarity_matrix > similarity_threshold).float()

            # Remove self-loops and ensure symmetry
            A_r.fill_diagonal_(0)
            A_r = (A_r + A_r.t()) / 2

            # Normalize adjacency matrix (D^(-1/2) * A * D^(-1/2))
            degree = A_r.sum(dim=1)
            degree[degree == 0] = 1  # Avoid division by zero
            D_inv_sqrt = torch.diag(torch.pow(degree, -0.5))
            A_r_normalized = torch.mm(torch.mm(D_inv_sqrt, A_r), D_inv_sqrt)

            adjacency_matrices[r] = A_r_normalized

        return adjacency_matrices

    def update_weights_with_gru(self, X_new):
        """
        Update GCN weight matrices using GRU based on new column features

        Args:
            X_new: (feature_dim,) aggregated features of new columns
        """
        X_new = X_new.unsqueeze(0)  # (1, feature_dim) for GRU input

        for layer_idx in range(self.num_layers):
            for r in self.relation_types:
                gru_name = f'layer_{layer_idx}_{r}'
                weight_name = f'layer_{layer_idx}_{r}'

                # Get current weight matrix and flatten it
                current_weight = self.gcn_weights[weight_name]
                flattened_weight = current_weight.view(-1).unsqueeze(0)  # (1, weight_size)

                # Update weight using GRU
                new_flattened_weight = self.weight_grus[gru_name](X_new, flattened_weight)

                # Reshape back to original weight matrix shape
                new_weight = new_flattened_weight.view(current_weight.shape)

                # Update the parameter
                self.gcn_weights[weight_name].data = new_weight.squeeze(0)

    def gcn_forward(self, H, adjacency_matrices):
        """
        Multi-relational GCN forward pass

        Args:
            H: (N, feature_dim) node features
            adjacency_matrices: Dict of adjacency matrices for each relation

        Returns:
            Updated node embeddings
        """
        for layer_idx in range(self.num_layers):
            H_next = torch.zeros(H.size(0),
                                 self.hidden_dim if layer_idx == 0 else self.output_dim,
                                 device=self.device)

            # Aggregate over all relation types
            for r in self.relation_types:
                if r in adjacency_matrices:
                    A_r = adjacency_matrices[r]
                    W_r = self.gcn_weights[f'layer_{layer_idx}_{r}']

                    # GCN propagation: A * H * W
                    H_r = torch.mm(torch.mm(A_r, H), W_r)
                    H_next += H_r

            # Apply activation (ReLU for hidden layers, identity for output layer)
            if layer_idx < self.num_layers - 1:
                H = self.activation(H_next)
            else:
                H = H_next

        return H

    def forward(self, adjacency_matrices, node_features, X_new=None):
        """
        EGCN forward pass - generates high-quality node embeddings

        Args:
            adjacency_matrices: Dict of adjacency matrices for each relation type
                               {'coal': tensor, 'time': tensor, 'vessel': tensor}
            node_features: (N, feature_dim) current node features
            X_new: (feature_dim,) aggregated features of new columns (optional, for GRU update)

        Returns:
            node_embeddings: (N, output_dim) high-quality node embeddings
            adjacency_matrices: Dict of adjacency matrices (for compatibility)
        """
        # Step 1: Update GCN weights if new columns are provided
        if X_new is not None:
            self.update_weights_with_gru(X_new)

        # Step 2: Apply multi-relational GCN to generate embeddings
        node_embeddings = self.gcn_forward(node_features, adjacency_matrices)

        return node_embeddings, adjacency_matrices


# Keep the original classes for backward compatibility
class GRCU(torch.nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        cell_args = u.Namespace({})
        cell_args.rows = args.in_feats
        cell_args.cols = args.out_feats

        self.evolve_weights = mat_GRU_cell(cell_args)

        self.activation = self.args.activation
        self.GCN_init_weights = Parameter(torch.Tensor(self.args.in_feats, self.args.out_feats))
        self.reset_param(self.GCN_init_weights)

    def reset_param(self, t):
        # Initialize based on the number of columns
        stdv = 1. / math.sqrt(t.size(1))
        t.data.uniform_(-stdv, stdv)

    def forward(self, A_list, node_embs_list):  # ,mask_list):
        GCN_weights = self.GCN_init_weights
        out_seq = []
        for t, Ahat in enumerate(A_list):
            node_embs = node_embs_list[t]
            # first evolve the weights from the initial and use the new weights with the node_embs
            GCN_weights = self.evolve_weights(GCN_weights)  # ,node_embs,mask_list[t])
            node_embs = self.activation(Ahat.matmul(node_embs.matmul(GCN_weights)))

            out_seq.append(node_embs)

        return out_seq


class mat_GRU_cell(torch.nn.Module):
    def __init__(self, args):
        super().__init__()
        self.args = args
        self.update = mat_GRU_gate(args.rows,
                                   args.cols,
                                   torch.nn.Sigmoid())

        self.reset = mat_GRU_gate(args.rows,
                                  args.cols,
                                  torch.nn.Sigmoid())

        self.htilda = mat_GRU_gate(args.rows,
                                   args.cols,
                                   torch.nn.Tanh())

        self.choose_topk = TopK(feats=args.rows,
                                k=args.cols)

    def forward(self, prev_Q):  # ,prev_Z,mask):
        # z_topk = self.choose_topk(prev_Z,mask)
        z_topk = prev_Q

        update = self.update(z_topk, prev_Q)
        reset = self.reset(z_topk, prev_Q)

        h_cap = reset * prev_Q
        h_cap = self.htilda(z_topk, h_cap)

        new_Q = (1 - update) * prev_Q + update * h_cap

        return new_Q


class mat_GRU_gate(torch.nn.Module):
    def __init__(self, rows, cols, activation):
        super().__init__()
        self.activation = activation
        # the k here should be in_feats which is actually the rows
        self.W = Parameter(torch.Tensor(rows, rows))
        self.reset_param(self.W)

        self.U = Parameter(torch.Tensor(rows, rows))
        self.reset_param(self.U)

        self.bias = Parameter(torch.zeros(rows, cols))

    def reset_param(self, t):
        # Initialize based on the number of columns
        stdv = 1. / math.sqrt(t.size(1))
        t.data.uniform_(-stdv, stdv)

    def forward(self, x, hidden):
        out = self.activation(self.W.matmul(x) + \
                              self.U.matmul(hidden) + \
                              self.bias)

        return out


class TopK(torch.nn.Module):
    def __init__(self, feats, k):
        super().__init__()
        self.scorer = Parameter(torch.Tensor(feats, 1))
        self.reset_param(self.scorer)

        self.k = k

    def reset_param(self, t):
        # Initialize based on the number of rows
        stdv = 1. / math.sqrt(t.size(0))
        t.data.uniform_(-stdv, stdv)

    def forward(self, node_embs, mask):
        scores = node_embs.matmul(self.scorer) / self.scorer.norm()
        scores = scores + mask

        vals, topk_indices = scores.view(-1).topk(self.k)
        topk_indices = topk_indices[vals > -float("Inf")]

        if topk_indices.size(0) < self.k:
            topk_indices = u.pad_with_last_val(topk_indices, self.k)

        tanh = torch.nn.Tanh()

        if isinstance(node_embs, torch.sparse.FloatTensor) or \
                isinstance(node_embs, torch.cuda.sparse.FloatTensor):
            node_embs = node_embs.to_dense()

        out = node_embs[topk_indices] * tanh(scores[topk_indices].view(-1, 1))

        # we need to transpose the output
        return out.t()
